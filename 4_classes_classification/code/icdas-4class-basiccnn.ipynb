{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7617742,"sourceType":"datasetVersion","datasetId":4436735}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":6127.926056,"end_time":"2024-02-13T00:49:21.768635","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-12T23:07:13.842579","version":"2.5.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport numpy\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n#%matplotlib inline\n\nimport gc\n\nfrom torch.optim.lr_scheduler import StepLR\n\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\n\nfrom torchvision.models.vision_transformer import VisionTransformer\n\nfrom pytorch_pretrained_vit import ViT\n\n \n# Import label encoder \nfrom sklearn import preprocessing ","metadata":{"id":"SHJDyiS7PQ7v","papermill":{"duration":3.854551,"end_time":"2024-02-12T23:07:40.385504","exception":false,"start_time":"2024-02-12T23:07:36.530953","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:06.640605Z","iopub.execute_input":"2024-02-13T14:14:06.640886Z","iopub.status.idle":"2024-02-13T14:14:09.711795Z","shell.execute_reply.started":"2024-02-13T14:14:06.640860Z","shell.execute_reply":"2024-02-13T14:14:09.710867Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(20)","metadata":{"id":"wSKVDATbPcwZ","outputId":"64c7300b-f3c4-4982-d5b4-abd16964d240","papermill":{"duration":0.020196,"end_time":"2024-02-12T23:07:40.415242","exception":false,"start_time":"2024-02-12T23:07:40.395046","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:09.712964Z","iopub.execute_input":"2024-02-13T14:14:09.713390Z","iopub.status.idle":"2024-02-13T14:14:09.722209Z","shell.execute_reply.started":"2024-02-13T14:14:09.713364Z","shell.execute_reply":"2024-02-13T14:14:09.721217Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c091f295b30>"},"metadata":{}}]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")","metadata":{"id":"JVjymfmCPm4X","outputId":"ea76c4fb-1c91-49a5-e1d8-951d9e79df4d","papermill":{"duration":0.076601,"end_time":"2024-02-12T23:07:40.500543","exception":false,"start_time":"2024-02-12T23:07:40.423942","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:09.724836Z","iopub.execute_input":"2024-02-13T14:14:09.725388Z","iopub.status.idle":"2024-02-13T14:14:09.756177Z","shell.execute_reply.started":"2024-02-13T14:14:09.725343Z","shell.execute_reply":"2024-02-13T14:14:09.755301Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}]},{"cell_type":"code","source":"# Directory Names\ndir_training = '/kaggle/input/icdas-70x70/icdas_preprocessed/training'\ndir_testing = '/kaggle/input/icdas-70x70/icdas_preprocessed/testing'","metadata":{"id":"Xyi6_qudPoGX","papermill":{"duration":0.015502,"end_time":"2024-02-12T23:07:40.525102","exception":false,"start_time":"2024-02-12T23:07:40.509600","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:09.757203Z","iopub.execute_input":"2024-02-13T14:14:09.757472Z","iopub.status.idle":"2024-02-13T14:14:09.769453Z","shell.execute_reply.started":"2024-02-13T14:14:09.757441Z","shell.execute_reply":"2024-02-13T14:14:09.768622Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass ToothDataset(Dataset):\n    def __init__(self, img_dir, transform=None):\n        self.dataset_path = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(os.listdir(self.dataset_path))\n\n    def __getitem__(self, idx):\n        if idx >= len(os.listdir(self.dataset_path)):\n            print(\"No datafile/image at index: \" + str(idx))\n            return None\n        \n        npy_filename = os.listdir(self.dataset_path)[idx]\n        label = int(npy_filename[-5]) - 3  # Extract the last digit and convert to class label\n        \n        numpy_arr = numpy.load(self.dataset_path + '/' + npy_filename)\n        \n        for i in range(numpy_arr.shape[0] - 70):\n            numpy_arr = numpy.delete(numpy_arr, [0], axis=0)\n            \n        numpy_arr = numpy_arr.reshape(1, 70, 70, 70)\n        tensor_arr = torch.from_numpy(numpy_arr).to(torch.float32)\n\n        del numpy_arr \n        gc.collect()\n        \n        if self.transform:\n            tensor_arr = self.transform(tensor_arr)  # Apply transformations\n            \n        return tensor_arr.to(torch.float32), torch.tensor(label)\n","metadata":{"id":"HYJagPZOQeeM","papermill":{"duration":0.021143,"end_time":"2024-02-12T23:07:40.555008","exception":false,"start_time":"2024-02-12T23:07:40.533865","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:09.770492Z","iopub.execute_input":"2024-02-13T14:14:09.770742Z","iopub.status.idle":"2024-02-13T14:14:09.781178Z","shell.execute_reply.started":"2024-02-13T14:14:09.770720Z","shell.execute_reply":"2024-02-13T14:14:09.780490Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"training_data = ToothDataset(img_dir=dir_training, transform=None)\nvalidation_data = ToothDataset(img_dir=dir_testing, transform=None)","metadata":{"id":"Nr398yHVQhxT","papermill":{"duration":0.015492,"end_time":"2024-02-12T23:07:40.629055","exception":false,"start_time":"2024-02-12T23:07:40.613563","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:09.782308Z","iopub.execute_input":"2024-02-13T14:14:09.782662Z","iopub.status.idle":"2024-02-13T14:14:09.790169Z","shell.execute_reply.started":"2024-02-13T14:14:09.782632Z","shell.execute_reply":"2024-02-13T14:14:09.789360Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Basic3DCNN(nn.Module):\n    def __init__(self, num_classes=4):\n        super(Basic3DCNN, self).__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)\n        # Max pooling layers\n        self.pool = nn.MaxPool3d(2, 2)\n        # Fully connected layers\n        self.fc1 = nn.Linear(64 * 8 * 8 * 8, 256)  # Adjust input size based on pooling\n        self.dropout = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        # Convolutional layers with ReLU activation and max pooling\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        # Flatten the output\n        x = x.view(-1, 64 * 8 * 8 * 8)  # Adjust output size based on pooling\n        # Fully connected layers with ReLU activation\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T14:14:10.055701Z","iopub.execute_input":"2024-02-13T14:14:10.055994Z","iopub.status.idle":"2024-02-13T14:14:10.066045Z","shell.execute_reply.started":"2024-02-13T14:14:10.055970Z","shell.execute_reply":"2024-02-13T14:14:10.065195Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# model = NeuralNetwork().to(device)\nmodel = Basic3DCNN().to(device)","metadata":{"id":"sRE_KuDDQnKG","papermill":{"duration":0.313324,"end_time":"2024-02-12T23:07:45.010968","exception":false,"start_time":"2024-02-12T23:07:44.697644","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:10.080812Z","iopub.execute_input":"2024-02-13T14:14:10.081058Z","iopub.status.idle":"2024-02-13T14:14:10.317669Z","shell.execute_reply.started":"2024-02-13T14:14:10.081028Z","shell.execute_reply":"2024-02-13T14:14:10.316857Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"datax = training_data[0][0].reshape(1,1,70,70,70).to(device)\ndatax.shape","metadata":{"id":"m7PQHq3TmJyX","papermill":{"duration":0.157733,"end_time":"2024-02-12T23:07:45.207435","exception":false,"start_time":"2024-02-12T23:07:45.049702","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:10.324831Z","iopub.execute_input":"2024-02-13T14:14:10.325381Z","iopub.status.idle":"2024-02-13T14:14:10.504536Z","shell.execute_reply.started":"2024-02-13T14:14:10.325349Z","shell.execute_reply":"2024-02-13T14:14:10.503628Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 1, 70, 70, 70])"},"metadata":{}}]},{"cell_type":"code","source":"model(datax)","metadata":{"id":"tszA-R18lGgS","outputId":"25e24891-937d-4122-aa8d-98d18200cb8f","papermill":{"duration":0.923478,"end_time":"2024-02-12T23:07:46.141458","exception":false,"start_time":"2024-02-12T23:07:45.217980","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:10.505728Z","iopub.execute_input":"2024-02-13T14:14:10.506127Z","iopub.status.idle":"2024-02-13T14:14:11.296430Z","shell.execute_reply.started":"2024-02-13T14:14:10.506091Z","shell.execute_reply":"2024-02-13T14:14:11.295560Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.0347,  0.0103,  0.0212,  0.0087]], device='cuda:0',\n       grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"# Hyperparameters\nepochs = 500\nbatch_size = 2\nlearning_rate = 1e-3\nweight_decay = 0.0000000001\nmomentum=0.9","metadata":{"id":"Rju3Y47LQoaJ","papermill":{"duration":0.017182,"end_time":"2024-02-12T23:07:46.169103","exception":false,"start_time":"2024-02-12T23:07:46.151921","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:14.345619Z","iopub.execute_input":"2024-02-13T14:14:14.346365Z","iopub.status.idle":"2024-02-13T14:14:14.350741Z","shell.execute_reply.started":"2024-02-13T14:14:14.346313Z","shell.execute_reply":"2024-02-13T14:14:14.349745Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"loss_function=nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam( model.parameters()  ,lr=learning_rate)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"id":"iTnpPGvRQqiu","papermill":{"duration":0.018733,"end_time":"2024-02-12T23:07:46.198001","exception":false,"start_time":"2024-02-12T23:07:46.179268","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:15.237438Z","iopub.execute_input":"2024-02-13T14:14:15.238194Z","iopub.status.idle":"2024-02-13T14:14:15.243253Z","shell.execute_reply.started":"2024-02-13T14:14:15.238164Z","shell.execute_reply":"2024-02-13T14:14:15.242372Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"training_data_loader = DataLoader(training_data, batch_size, shuffle = True)\nvalidation_data_loader = DataLoader(validation_data, batch_size, shuffle = False)","metadata":{"id":"GINVboKiQr4k","papermill":{"duration":0.017611,"end_time":"2024-02-12T23:07:46.226855","exception":false,"start_time":"2024-02-12T23:07:46.209244","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:16.138691Z","iopub.execute_input":"2024-02-13T14:14:16.139306Z","iopub.status.idle":"2024-02-13T14:14:16.145086Z","shell.execute_reply.started":"2024-02-13T14:14:16.139275Z","shell.execute_reply":"2024-02-13T14:14:16.144191Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def train(dataloader, model, loss_fn, optimizer):\n    torch.cuda.empty_cache()\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n        \n        # Compute prediction error\n        pred = model(X)\n        \n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch%5==0:\n          # Print\n          loss, current = loss.item(), batch * len(X)\n          print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")","metadata":{"id":"j-XRPMtkQtM7","papermill":{"duration":0.019395,"end_time":"2024-02-12T23:07:46.256465","exception":false,"start_time":"2024-02-12T23:07:46.237070","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:17.186289Z","iopub.execute_input":"2024-02-13T14:14:17.186943Z","iopub.status.idle":"2024-02-13T14:14:17.193934Z","shell.execute_reply.started":"2024-02-13T14:14:17.186913Z","shell.execute_reply":"2024-02-13T14:14:17.192941Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"validation_accuracy = []\ndef validation(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n#             print('validation pred: ', pred, ' validation y: ', y.squeeze())\n            test_loss += loss_fn(pred, y).item()\n            correct += (torch.argmax(pred, dim=1) == y.squeeze()).sum().item()\n            X.cpu()\n            y.cpu()\n    test_loss /= num_batches\n    correct /= size\n    validation_accuracy.append(correct*100)\n    # Print\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n    return 100*correct","metadata":{"id":"qHD2Fgd2Qum-","papermill":{"duration":0.01971,"end_time":"2024-02-12T23:07:46.286375","exception":false,"start_time":"2024-02-12T23:07:46.266665","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:19.169229Z","iopub.execute_input":"2024-02-13T14:14:19.169600Z","iopub.status.idle":"2024-02-13T14:14:19.177824Z","shell.execute_reply.started":"2024-02-13T14:14:19.169570Z","shell.execute_reply":"2024-02-13T14:14:19.176845Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"\n# Define the directory to save the model\nsave_dir = \"/kaggle/working/saved_models\"\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\nbest_accuracy = 0.0  # Initialize best validation accuracy\nbest_epoch = 0  # Initialize the epoch with the best validation accuracy\n\n# Training\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(training_data_loader, model, loss_function, optimizer)\n    accuracy = validation(validation_data_loader, model, loss_function)\n    \n#     scheduler.step()\n    \n    # Save the model for every 50th epoch\n    if (t + 1) % 100 == 0:\n        save_path = os.path.join(save_dir, f\"model_epoch_{t+1}_accuracy_{accuracy:.2f}.pt\")\n        torch.save(model.state_dict(), save_path)\n        print(f\"Model saved at epoch {t+1}\")\n    \n    # Check if the current accuracy is better than the best accuracy\n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        best_epoch = t + 1\n        best_model_path = os.path.join(save_dir, f\"best_model_epoch_{best_epoch}_accuracy_{best_accuracy:.2f}.pt\")\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"Best model saved with accuracy: {best_accuracy:.2f} at epoch {best_epoch}\")\n\nprint(\"Training done!\")\nprint(f\"Best validation accuracy: {best_accuracy:.2f} at epoch {best_epoch}\")","metadata":{"id":"YCMkki52Qx9_","outputId":"bf0e17e8-37c3-4b66-c1bd-2ab0502ce9aa","papermill":{"duration":6092.502448,"end_time":"2024-02-13T00:49:18.825918","exception":false,"start_time":"2024-02-12T23:07:46.323470","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-13T14:14:20.168577Z","iopub.execute_input":"2024-02-13T14:14:20.168922Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1\n-------------------------------\nloss: 1.406562  [    0/   38]\nloss: 1.490210  [   10/   38]\nloss: 1.394636  [   20/   38]\nloss: 1.400616  [   30/   38]\nTest Error: \n Accuracy: 27.8%, Avg loss: 1.385720 \n\nBest model saved with accuracy: 27.78 at epoch 1\nEpoch 2\n-------------------------------\nloss: 1.379091  [    0/   38]\nloss: 1.377698  [   10/   38]\nloss: 1.394830  [   20/   38]\nloss: 1.426998  [   30/   38]\nTest Error: \n Accuracy: 27.8%, Avg loss: 1.385518 \n\nEpoch 3\n-------------------------------\nloss: 1.380087  [    0/   38]\nloss: 1.394438  [   10/   38]\nloss: 1.395897  [   20/   38]\nloss: 1.394661  [   30/   38]\nTest Error: \n Accuracy: 27.8%, Avg loss: 1.385690 \n\nEpoch 4\n-------------------------------\nloss: 1.363633  [    0/   38]\nloss: 1.409135  [   10/   38]\nloss: 1.424318  [   20/   38]\nloss: 1.393775  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.385845 \n\nEpoch 5\n-------------------------------\nloss: 1.393631  [    0/   38]\nloss: 1.392402  [   10/   38]\nloss: 1.391365  [   20/   38]\nloss: 1.389971  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.386205 \n\nEpoch 6\n-------------------------------\nloss: 1.361006  [    0/   38]\nloss: 1.383312  [   10/   38]\nloss: 1.393825  [   20/   38]\nloss: 1.393361  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.386280 \n\nEpoch 7\n-------------------------------\nloss: 1.362527  [    0/   38]\nloss: 1.393513  [   10/   38]\nloss: 1.384005  [   20/   38]\nloss: 1.362827  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.386396 \n\nEpoch 8\n-------------------------------\nloss: 1.393237  [    0/   38]\nloss: 1.379720  [   10/   38]\nloss: 1.405014  [   20/   38]\nloss: 1.416811  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.386458 \n\nEpoch 9\n-------------------------------\nloss: 1.366982  [    0/   38]\nloss: 1.380964  [   10/   38]\nloss: 1.411030  [   20/   38]\nloss: 1.377410  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.386720 \n\nEpoch 10\n-------------------------------\nloss: 1.392467  [    0/   38]\nloss: 1.387807  [   10/   38]\nloss: 1.386137  [   20/   38]\nloss: 1.412594  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.386641 \n\nEpoch 11\n-------------------------------\nloss: 1.380286  [    0/   38]\nloss: 1.405951  [   10/   38]\nloss: 1.380950  [   20/   38]\nloss: 1.368630  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.386943 \n\nEpoch 12\n-------------------------------\nloss: 1.404344  [    0/   38]\nloss: 1.389628  [   10/   38]\nloss: 1.403490  [   20/   38]\nloss: 1.369633  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.386898 \n\nEpoch 13\n-------------------------------\nloss: 1.379338  [    0/   38]\nloss: 1.357679  [   10/   38]\nloss: 1.404688  [   20/   38]\nloss: 1.395330  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.387482 \n\nEpoch 14\n-------------------------------\nloss: 1.379051  [    0/   38]\nloss: 1.405449  [   10/   38]\nloss: 1.377266  [   20/   38]\nloss: 1.380625  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.387407 \n\nEpoch 15\n-------------------------------\nloss: 1.404139  [    0/   38]\nloss: 1.406616  [   10/   38]\nloss: 1.396373  [   20/   38]\nloss: 1.355455  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.387402 \n\nEpoch 16\n-------------------------------\nloss: 1.380889  [    0/   38]\nloss: 1.355910  [   10/   38]\nloss: 1.393226  [   20/   38]\nloss: 1.368333  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.387681 \n\nEpoch 17\n-------------------------------\nloss: 1.379112  [    0/   38]\nloss: 1.377072  [   10/   38]\nloss: 1.354648  [   20/   38]\nloss: 1.407656  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.387734 \n\nEpoch 18\n-------------------------------\nloss: 1.386168  [    0/   38]\nloss: 1.377056  [   10/   38]\nloss: 1.368701  [   20/   38]\nloss: 1.375622  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.387958 \n\nEpoch 19\n-------------------------------\nloss: 1.404980  [    0/   38]\nloss: 1.374101  [   10/   38]\nloss: 1.389657  [   20/   38]\nloss: 1.415524  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.388293 \n\nEpoch 20\n-------------------------------\nloss: 1.390098  [    0/   38]\nloss: 1.373853  [   10/   38]\nloss: 1.383020  [   20/   38]\nloss: 1.393147  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.388580 \n\nEpoch 21\n-------------------------------\nloss: 1.366400  [    0/   38]\nloss: 1.348807  [   10/   38]\nloss: 1.401985  [   20/   38]\nloss: 1.351597  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.388356 \n\nEpoch 22\n-------------------------------\nloss: 1.368726  [    0/   38]\nloss: 1.369839  [   10/   38]\nloss: 1.405249  [   20/   38]\nloss: 1.374010  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.388502 \n\nEpoch 23\n-------------------------------\nloss: 1.404776  [    0/   38]\nloss: 1.387665  [   10/   38]\nloss: 1.405005  [   20/   38]\nloss: 1.401592  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.388600 \n\nEpoch 24\n-------------------------------\nloss: 1.374726  [    0/   38]\nloss: 1.381082  [   10/   38]\nloss: 1.384856  [   20/   38]\nloss: 1.400143  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.388802 \n\nEpoch 25\n-------------------------------\nloss: 1.374621  [    0/   38]\nloss: 1.412909  [   10/   38]\nloss: 1.392431  [   20/   38]\nloss: 1.366643  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.388834 \n\nEpoch 26\n-------------------------------\nloss: 1.404691  [    0/   38]\nloss: 1.407369  [   10/   38]\nloss: 1.365840  [   20/   38]\nloss: 1.411246  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.389142 \n\nEpoch 27\n-------------------------------\nloss: 1.388645  [    0/   38]\nloss: 1.411330  [   10/   38]\nloss: 1.400149  [   20/   38]\nloss: 1.380982  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.389102 \n\nEpoch 28\n-------------------------------\nloss: 1.399363  [    0/   38]\nloss: 1.373827  [   10/   38]\nloss: 1.346462  [   20/   38]\nloss: 1.392461  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.389599 \n\nEpoch 29\n-------------------------------\nloss: 1.341445  [    0/   38]\nloss: 1.375699  [   10/   38]\nloss: 1.375747  [   20/   38]\nloss: 1.413227  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.389760 \n\nEpoch 30\n-------------------------------\nloss: 1.374701  [    0/   38]\nloss: 1.381898  [   10/   38]\nloss: 1.398228  [   20/   38]\nloss: 1.389843  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.389645 \n\nEpoch 31\n-------------------------------\nloss: 1.376812  [    0/   38]\nloss: 1.390936  [   10/   38]\nloss: 1.347188  [   20/   38]\nloss: 1.377936  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.389456 \n\nEpoch 32\n-------------------------------\nloss: 1.412309  [    0/   38]\nloss: 1.411969  [   10/   38]\nloss: 1.411157  [   20/   38]\nloss: 1.345929  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.387902 \n\nEpoch 33\n-------------------------------\nloss: 1.398457  [    0/   38]\nloss: 1.348033  [   10/   38]\nloss: 1.387514  [   20/   38]\nloss: 1.393257  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.388311 \n\nEpoch 34\n-------------------------------\nloss: 1.425519  [    0/   38]\nloss: 1.280478  [   10/   38]\nloss: 1.415424  [   20/   38]\nloss: 1.385867  [   30/   38]\nTest Error: \n Accuracy: 22.2%, Avg loss: 1.558696 \n\nEpoch 35\n-------------------------------\nloss: 1.247555  [    0/   38]\nloss: 1.309677  [   10/   38]\nloss: 1.239573  [   20/   38]\nloss: 1.219034  [   30/   38]\nTest Error: \n Accuracy: 27.8%, Avg loss: 1.399491 \n\nEpoch 36\n-------------------------------\nloss: 1.582039  [    0/   38]\nloss: 1.372234  [   10/   38]\nloss: 1.411489  [   20/   38]\nloss: 1.262107  [   30/   38]\nTest Error: \n Accuracy: 55.6%, Avg loss: 1.360651 \n\nBest model saved with accuracy: 55.56 at epoch 36\nEpoch 37\n-------------------------------\nloss: 1.143826  [    0/   38]\nloss: 0.481194  [   10/   38]\nloss: 1.108162  [   20/   38]\nloss: 1.598225  [   30/   38]\nTest Error: \n Accuracy: 61.1%, Avg loss: 0.846317 \n\nBest model saved with accuracy: 61.11 at epoch 37\nEpoch 38\n-------------------------------\nloss: 0.249131  [    0/   38]\nloss: 0.127566  [   10/   38]\nloss: 0.411629  [   20/   38]\nloss: 0.393964  [   30/   38]\nTest Error: \n Accuracy: 61.1%, Avg loss: 0.928501 \n\nEpoch 39\n-------------------------------\nloss: 0.575594  [    0/   38]\nloss: 0.782803  [   10/   38]\nloss: 0.301626  [   20/   38]\nloss: 0.105341  [   30/   38]\nTest Error: \n Accuracy: 66.7%, Avg loss: 0.932512 \n\nBest model saved with accuracy: 66.67 at epoch 39\nEpoch 40\n-------------------------------\nloss: 0.013527  [    0/   38]\nloss: 0.154344  [   10/   38]\nloss: 0.055666  [   20/   38]\nloss: 0.025738  [   30/   38]\nTest Error: \n Accuracy: 72.2%, Avg loss: 0.809337 \n\nBest model saved with accuracy: 72.22 at epoch 40\nEpoch 41\n-------------------------------\nloss: 0.003131  [    0/   38]\nloss: 0.058366  [   10/   38]\nloss: 0.030533  [   20/   38]\nloss: 0.010328  [   30/   38]\nTest Error: \n Accuracy: 72.2%, Avg loss: 0.774773 \n\nEpoch 42\n-------------------------------\nloss: 0.006301  [    0/   38]\nloss: 0.666733  [   10/   38]\nloss: 0.049858  [   20/   38]\nloss: 0.012783  [   30/   38]\nTest Error: \n Accuracy: 72.2%, Avg loss: 0.783424 \n\nEpoch 43\n-------------------------------\nloss: 0.070067  [    0/   38]\nloss: 0.002216  [   10/   38]\nloss: 0.000047  [   20/   38]\nloss: 0.000015  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.395818 \n\nBest model saved with accuracy: 88.89 at epoch 43\nEpoch 44\n-------------------------------\nloss: 0.005317  [    0/   38]\nloss: 0.000219  [   10/   38]\nloss: 0.012970  [   20/   38]\nloss: 0.000611  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.360512 \n\nEpoch 45\n-------------------------------\nloss: 0.005329  [    0/   38]\nloss: 0.000074  [   10/   38]\nloss: 0.002023  [   20/   38]\nloss: 0.000638  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.390533 \n\nEpoch 46\n-------------------------------\nloss: 0.000009  [    0/   38]\nloss: 0.003107  [   10/   38]\nloss: 0.000043  [   20/   38]\nloss: 0.000408  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.401548 \n\nEpoch 47\n-------------------------------\nloss: 0.000099  [    0/   38]\nloss: 0.000077  [   10/   38]\nloss: 0.000057  [   20/   38]\nloss: 0.000328  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.423973 \n\nEpoch 48\n-------------------------------\nloss: 0.000076  [    0/   38]\nloss: 0.000064  [   10/   38]\nloss: 0.000149  [   20/   38]\nloss: 0.000417  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.442719 \n\nEpoch 49\n-------------------------------\nloss: 0.000841  [    0/   38]\nloss: 0.000012  [   10/   38]\nloss: 0.000033  [   20/   38]\nloss: 0.000392  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.445518 \n\nEpoch 50\n-------------------------------\nloss: 0.000012  [    0/   38]\nloss: 0.000142  [   10/   38]\nloss: 0.000013  [   20/   38]\nloss: 0.000207  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.403126 \n\nEpoch 51\n-------------------------------\nloss: 0.000002  [    0/   38]\nloss: 0.000637  [   10/   38]\nloss: 0.000050  [   20/   38]\nloss: 0.000164  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.347075 \n\nEpoch 52\n-------------------------------\nloss: 0.000719  [    0/   38]\nloss: 0.002225  [   10/   38]\nloss: 0.000058  [   20/   38]\nloss: 0.001659  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.372332 \n\nEpoch 53\n-------------------------------\nloss: 0.000010  [    0/   38]\nloss: 0.000286  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000080  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.360177 \n\nEpoch 54\n-------------------------------\nloss: 0.000083  [    0/   38]\nloss: 0.000015  [   10/   38]\nloss: 0.000031  [   20/   38]\nloss: 0.000001  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.352872 \n\nEpoch 55\n-------------------------------\nloss: 0.000004  [    0/   38]\nloss: 0.000075  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000154  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.351931 \n\nEpoch 56\n-------------------------------\nloss: 0.005560  [    0/   38]\nloss: 0.000052  [   10/   38]\nloss: 0.000128  [   20/   38]\nloss: 0.003177  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.352311 \n\nEpoch 57\n-------------------------------\nloss: 0.000005  [    0/   38]\nloss: 0.000079  [   10/   38]\nloss: 0.000010  [   20/   38]\nloss: 0.000111  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.345658 \n\nEpoch 58\n-------------------------------\nloss: 0.000156  [    0/   38]\nloss: 0.000199  [   10/   38]\nloss: 0.000126  [   20/   38]\nloss: 0.000035  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.348703 \n\nEpoch 59\n-------------------------------\nloss: 0.000045  [    0/   38]\nloss: 0.000004  [   10/   38]\nloss: 0.000112  [   20/   38]\nloss: 0.000003  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.347688 \n\nEpoch 60\n-------------------------------\nloss: 0.000021  [    0/   38]\nloss: 0.000021  [   10/   38]\nloss: 0.000062  [   20/   38]\nloss: 0.000006  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.364021 \n\nEpoch 61\n-------------------------------\nloss: 0.000001  [    0/   38]\nloss: 0.000186  [   10/   38]\nloss: 0.000001  [   20/   38]\nloss: 0.000198  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.348245 \n\nEpoch 62\n-------------------------------\nloss: 0.000085  [    0/   38]\nloss: 0.000001  [   10/   38]\nloss: 0.000014  [   20/   38]\nloss: 0.000146  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.330219 \n\nEpoch 63\n-------------------------------\nloss: 0.000248  [    0/   38]\nloss: 0.000011  [   10/   38]\nloss: 0.000020  [   20/   38]\nloss: 0.000010  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.344840 \n\nBest model saved with accuracy: 94.44 at epoch 63\nEpoch 64\n-------------------------------\nloss: 0.000035  [    0/   38]\nloss: 0.000001  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000012  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.330231 \n\nEpoch 65\n-------------------------------\nloss: 0.000080  [    0/   38]\nloss: 0.000119  [   10/   38]\nloss: 0.000005  [   20/   38]\nloss: 0.000002  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.315021 \n\nEpoch 66\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000039  [   10/   38]\nloss: 0.000006  [   20/   38]\nloss: 0.000012  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.298951 \n\nEpoch 67\n-------------------------------\nloss: 0.000396  [    0/   38]\nloss: 0.000008  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000106  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.325376 \n\nEpoch 68\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000010  [   10/   38]\nloss: 0.000003  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.315117 \n\nEpoch 69\n-------------------------------\nloss: 0.000002  [    0/   38]\nloss: 0.000603  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000043  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.340397 \n\nEpoch 70\n-------------------------------\nloss: 0.000023  [    0/   38]\nloss: 0.000004  [   10/   38]\nloss: 0.000019  [   20/   38]\nloss: 0.000001  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.340952 \n\nEpoch 71\n-------------------------------\nloss: 0.000004  [    0/   38]\nloss: 0.000458  [   10/   38]\nloss: 0.000002  [   20/   38]\nloss: 0.000001  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.308286 \n\nEpoch 72\n-------------------------------\nloss: 0.000003  [    0/   38]\nloss: 0.000052  [   10/   38]\nloss: 0.000002  [   20/   38]\nloss: 0.000041  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.297657 \n\nEpoch 73\n-------------------------------\nloss: 0.000003  [    0/   38]\nloss: 0.000020  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.266594 \n\nEpoch 74\n-------------------------------\nloss: 0.000004  [    0/   38]\nloss: 0.000002  [   10/   38]\nloss: 0.000001  [   20/   38]\nloss: 0.000027  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.312279 \n\nEpoch 75\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000037  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000004  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.316683 \n\nEpoch 76\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000002  [   10/   38]\nloss: 0.000591  [   20/   38]\nloss: 0.000001  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.296412 \n\nEpoch 77\n-------------------------------\nloss: 0.000002  [    0/   38]\nloss: 0.000004  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.302883 \n\nEpoch 78\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000806  [   10/   38]\nloss: 0.000028  [   20/   38]\nloss: 0.000002  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.331124 \n\nEpoch 79\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000018  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.338507 \n\nEpoch 80\n-------------------------------\nloss: 0.000001  [    0/   38]\nloss: 0.000001  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.348302 \n\nEpoch 81\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000004  [   10/   38]\nloss: 0.000034  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.330800 \n\nEpoch 82\n-------------------------------\nloss: 0.000003  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000008  [   20/   38]\nloss: 0.000004  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.345313 \n\nEpoch 83\n-------------------------------\nloss: 0.000003  [    0/   38]\nloss: 0.000002  [   10/   38]\nloss: 0.000002  [   20/   38]\nloss: 0.000002  [   30/   38]\nTest Error: \n Accuracy: 77.8%, Avg loss: 0.351572 \n\nEpoch 84\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000002  [   10/   38]\nloss: 0.000077  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 77.8%, Avg loss: 0.337287 \n\nEpoch 85\n-------------------------------\nloss: 0.000006  [    0/   38]\nloss: 0.000001  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000001  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.319498 \n\nEpoch 86\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000002  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.309287 \n\nEpoch 87\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.292658 \n\nEpoch 88\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000001  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000025  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.281457 \n\nEpoch 89\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000004  [   10/   38]\nloss: 0.000001  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.302042 \n\nEpoch 90\n-------------------------------\nloss: 0.000003  [    0/   38]\nloss: 0.000003  [   10/   38]\nloss: 0.000146  [   20/   38]\nloss: 0.000001  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.257863 \n\nEpoch 91\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000003  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.254158 \n\nEpoch 92\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000003  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.263185 \n\nEpoch 93\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000024  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.281493 \n\nEpoch 94\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000005  [   20/   38]\nloss: 0.000013  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.323444 \n\nEpoch 95\n-------------------------------\nloss: 0.000001  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000001  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.323513 \n\nEpoch 96\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000002  [   10/   38]\nloss: 0.000181  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.290635 \n\nEpoch 97\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000011  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.307735 \n\nEpoch 98\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000002  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.279589 \n\nEpoch 99\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000001  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.314536 \n\nEpoch 100\n-------------------------------\nloss: 0.000003  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000002  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.311374 \n\nModel saved at epoch 100\nEpoch 101\n-------------------------------\nloss: 0.000003  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.347523 \n\nEpoch 102\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000012  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.331316 \n\nEpoch 103\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000002  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.319635 \n\nEpoch 104\n-------------------------------\nloss: 0.000001  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.267890 \n\nEpoch 105\n-------------------------------\nloss: 0.000003  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000003  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.281352 \n\nEpoch 106\n-------------------------------\nloss: 0.000001  [    0/   38]\nloss: 0.001129  [   10/   38]\nloss: 0.000040  [   20/   38]\nloss: 0.000002  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.499261 \n\nEpoch 107\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000013  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000041  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.246774 \n\nEpoch 108\n-------------------------------\nloss: 0.000002  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.346000 \n\nEpoch 109\n-------------------------------\nloss: 0.000009  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000011  [   20/   38]\nloss: 0.000002  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.475813 \n\nEpoch 110\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000002  [   30/   38]\nTest Error: \n Accuracy: 77.8%, Avg loss: 0.430487 \n\nEpoch 111\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000003  [   10/   38]\nloss: 0.000001  [   20/   38]\nloss: 0.000002  [   30/   38]\nTest Error: \n Accuracy: 77.8%, Avg loss: 0.431541 \n\nEpoch 112\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000002  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000014  [   30/   38]\nTest Error: \n Accuracy: 77.8%, Avg loss: 0.419601 \n\nEpoch 113\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000001  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.376511 \n\nEpoch 114\n-------------------------------\nloss: 0.000005  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.357263 \n\nEpoch 115\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000002  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.354314 \n\nEpoch 116\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000042  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.337185 \n\nEpoch 117\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000002  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.331252 \n\nEpoch 118\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000001  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.322107 \n\nEpoch 119\n-------------------------------\nloss: 0.000001  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.312485 \n\nEpoch 120\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000001  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.303249 \n\nEpoch 121\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000003  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.297525 \n\nEpoch 122\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.289755 \n\nEpoch 123\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000001  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.254864 \n\nEpoch 124\n-------------------------------\nloss: 0.000004  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.329538 \n\nEpoch 125\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000177  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.211674 \n\nEpoch 126\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.191166 \n\nEpoch 127\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.189899 \n\nEpoch 128\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.189258 \n\nEpoch 129\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000001  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.189121 \n\nEpoch 130\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.189245 \n\nEpoch 131\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.189313 \n\nEpoch 132\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.190308 \n\nEpoch 133\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000003  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.192703 \n\nEpoch 134\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.193025 \n\nEpoch 135\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000003  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.192520 \n\nEpoch 136\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.193695 \n\nEpoch 137\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000002  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.191585 \n\nEpoch 138\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.227625 \n\nEpoch 139\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000163  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 83.3%, Avg loss: 0.229597 \n\nEpoch 140\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000002  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.203667 \n\nEpoch 141\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.204073 \n\nEpoch 142\n-------------------------------\nloss: 0.000001  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000002  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.204974 \n\nEpoch 143\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.204900 \n\nEpoch 144\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000001  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.204987 \n\nEpoch 145\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.197705 \n\nEpoch 146\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000001  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.191316 \n\nEpoch 147\n-------------------------------\nloss: 0.000001  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.189892 \n\nEpoch 148\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.189149 \n\nEpoch 149\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.189533 \n\nEpoch 150\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.189710 \n\nEpoch 151\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.189541 \n\nEpoch 152\n-------------------------------\nloss: 0.000003  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.190354 \n\nEpoch 153\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.190789 \n\nEpoch 154\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.192611 \n\nEpoch 155\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.194190 \n\nEpoch 156\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.194051 \n\nEpoch 157\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000001  [   10/   38]\nloss: 0.000001  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.193023 \n\nEpoch 158\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.190360 \n\nEpoch 159\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.188883 \n\nEpoch 160\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.188272 \n\nEpoch 161\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.188424 \n\nEpoch 162\n-------------------------------\nloss: 0.000002  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.189859 \n\nEpoch 163\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.190418 \n\nEpoch 164\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.191438 \n\nEpoch 165\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.190168 \n\nEpoch 166\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.189699 \n\nEpoch 167\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.191722 \n\nEpoch 168\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000007  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.192335 \n\nEpoch 169\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.185654 \n\nEpoch 170\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.185403 \n\nEpoch 171\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.185259 \n\nEpoch 172\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000009  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.189961 \n\nEpoch 173\n-------------------------------\nloss: 0.000001  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.198181 \n\nEpoch 174\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.199192 \n\nEpoch 175\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.200871 \n\nEpoch 176\n-------------------------------\nloss: 0.000001  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.201281 \n\nEpoch 177\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.201420 \n\nEpoch 178\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.200696 \n\nEpoch 179\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.199684 \n\nEpoch 180\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.200150 \n\nEpoch 181\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.201016 \n\nEpoch 182\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.200359 \n\nEpoch 183\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.200463 \n\nEpoch 184\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000006  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.201043 \n\nEpoch 185\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000003  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.202000 \n\nEpoch 186\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.202359 \n\nEpoch 187\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.202063 \n\nEpoch 188\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.197101 \n\nEpoch 189\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.198133 \n\nEpoch 190\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.199971 \n\nEpoch 191\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\nTest Error: \n Accuracy: 94.4%, Avg loss: 0.200660 \n\nEpoch 192\n-------------------------------\nloss: 0.000000  [    0/   38]\nloss: 0.000000  [   10/   38]\nloss: 0.000000  [   20/   38]\nloss: 0.000000  [   30/   38]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"abgBr-mVRqHh","papermill":{"duration":0.202157,"end_time":"2024-02-13T00:49:19.231333","exception":false,"start_time":"2024-02-13T00:49:19.029176","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.199997,"end_time":"2024-02-13T00:49:19.631605","exception":false,"start_time":"2024-02-13T00:49:19.431608","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.201272,"end_time":"2024-02-13T00:49:20.032035","exception":false,"start_time":"2024-02-13T00:49:19.830763","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}